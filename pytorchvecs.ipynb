{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import text data\n",
    "#english - subtask a\n",
    "dtf = pd.read_csv(r\"en_train_normalised.csv\", delimiter=\",\", na_filter=False, encoding=\"utf-8\")             #training data\n",
    "dtf_test = pd.read_csv(r\"en_test_normalised.csv\", delimiter=\",\", na_filter=False, encoding=\"utf-8\")         #testing data\n",
    "#dtf_test = pd.read_csv(r\"en_dev_normalised.csv\", delimiter=\",\", na_filter=False, encoding=\"utf-8\")         #development \n",
    "\n",
    "#spanish text\n",
    "#dtf = pd.read_csv(r\"es_train_normalised.csv\", delimiter=\",\", na_filter=False, encoding=\"utf-8\")\n",
    "#dtf_test = pd.read_csv(r\"es_test_normalised.csv\", delimiter=\",\", na_filter=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the glove embeddings\n",
    "path  =r\"glove.twitter.27B.200d.txt\"\n",
    "\n",
    "glove = pd.read_csv(path, sep=\" \", quoting=3, header=None, index_col=0)\n",
    "glove_embedding = {key: val.values for key, val in glove.T.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create embedding matrix - look-table for embeddings based on index\n",
    "def create_embedding_matrix(word_index,embedding_dict,dimension):\n",
    "  embedding_matrix=np.zeros((len(word_index)+1,dimension))      #maybe better to use rand??\n",
    " \n",
    "  for word,index in word_index.items():\n",
    "    if word in embedding_dict:\n",
    "      embedding_matrix[index]=embedding_dict[word]\n",
    "  return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the embedding matrix on the testing vocabulary\n",
    "text = dtf[\"text\"].tolist()\n",
    " \n",
    "tokenizer=tf.keras.preprocessing.text.Tokenizer(split=\" \")\n",
    "tokenizer.fit_on_texts(text)\n",
    " \n",
    "text_token=tokenizer.texts_to_sequences(text)\n",
    " \n",
    "embedding_matrix = create_embedding_matrix(tokenizer.word_index,embedding_dict=glove_embedding,dimension=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = embedding_matrix.shape[0]              #no. words in dataset --> 11400\n",
    "vector_size = embedding_matrix.shape[1]             #dimension of vectors --> 200\n",
    "\n",
    "embedding = nn.Embedding(num_embeddings=vocab_size,embedding_dim=vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise embedding layer using pre-trained weights\n",
    "embedding.weight=nn.Parameter(torch.tensor(embedding_matrix,dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convoNN(nn.ModuleList):\n",
    "    #def __init__(self, weights_matrix, hidden_size, number_feature):\n",
    "    def __init__(self, weights_matrix, hidden_size, number_feature):\n",
    "        super(convoNN, self).__init__()\n",
    "        # intialize embedding layer with required size\n",
    "        self.vocab_size = weights_matrix.shape[0]              #no. words in dataset\n",
    "        self.vector_size = weights_matrix.shape[1]             #dimension of vectors\n",
    "\n",
    "        #embedding\n",
    "        self.embedding_layer = nn.Embedding(self.vocab_size + 1, self.vector_size)\n",
    "        #initialise embedding layer using pre-trained weights\n",
    "        self.embedding_layer.weight=nn.Parameter(torch.tensor(weights_matrix,dtype=torch.float32))\n",
    "        #disable learning bc pre-trained\n",
    "        #self.embedding_layer.weight.requires_grad=False\n",
    "\n",
    "        self.stride = 2\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.seq_len = 13\n",
    "        self.out_size = 32\n",
    "\n",
    "        #kernels\n",
    "        self.kernel_1 = 2\n",
    "        self.kernel_2 = 3\n",
    "        self.kernel_3 = 4\n",
    "        self.kernel_4 = 5\n",
    "\n",
    "        #convolution layers\n",
    "        self.conv_1 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_1, self.stride)     \n",
    "        self.conv_2 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_2, self.stride)\n",
    "        self.conv_3 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_3, self.stride)\n",
    "        self.conv_4 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_4, self.stride)\n",
    "\n",
    "        #pooling layers\n",
    "        self.pooling_1 = nn.MaxPool1d(self.kernel_1, self.stride)\n",
    "        self.pooling_2 = nn.MaxPool1d(self.kernel_2, self.stride)\n",
    "        self.pooling_3 = nn.MaxPool1d(self.kernel_3, self.stride)\n",
    "        self.pooling_4 = nn.MaxPool1d(self.kernel_4, self.stride)\n",
    "\n",
    "        #feature layer\n",
    "        self.feature_layer = nn.Linear(number_feature, number_feature).float()\n",
    "\n",
    "        # combination layer\n",
    "        self.combined_layer = nn.Linear(self.in_features_fc()+number_feature, hidden_size).float()        \n",
    "        \n",
    "    \t#fully connected layer\n",
    "        #self.fc = nn.Linear(self.in_features_fc(), 1)              #without features\n",
    "        self.fc = nn.Linear(hidden_size, 1)                         #with features\n",
    "    \n",
    "    def in_features_fc(self):\n",
    "\n",
    "      self.embedding_size = 200\n",
    "      # Calculate size of convolved/pooled features for convolution_1/max_pooling_1 features\n",
    "      out_conv_1 = ((self.embedding_size - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n",
    "      out_conv_1 = math.floor(out_conv_1)\n",
    "      out_pool_1 = ((out_conv_1 - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n",
    "      out_pool_1 = math.floor(out_pool_1)\n",
    "      \n",
    "      # Calculate size of convolved/pooled features for convolution_2/max_pooling_2 features\n",
    "      out_conv_2 = ((self.embedding_size - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n",
    "      out_conv_2 = math.floor(out_conv_2)\n",
    "      out_pool_2 = ((out_conv_2 - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n",
    "      out_pool_2 = math.floor(out_pool_2)\n",
    "      \n",
    "      # Calculate size of convolved/pooled features for convolution_3/max_pooling_3 features\n",
    "      out_conv_3 = ((self.embedding_size - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n",
    "      out_conv_3 = math.floor(out_conv_3)\n",
    "      out_pool_3 = ((out_conv_3 - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n",
    "      out_pool_3 = math.floor(out_pool_3)\n",
    "      \n",
    "      # Calculate size of convolved/pooled features for convolution_4/max_pooling_4 features\n",
    "      out_conv_4 = ((self.embedding_size - 1 * (self.kernel_4 - 1) - 1) / self.stride) + 1\n",
    "      out_conv_4 = math.floor(out_conv_4)\n",
    "      out_pool_4 = ((out_conv_4 - 1 * (self.kernel_4 - 1) - 1) / self.stride) + 1\n",
    "      out_pool_4 = math.floor(out_pool_4)\n",
    "      \n",
    "      # Returns \"flattened\" vector (input for fully connected layer)\n",
    "      return (out_pool_1 + out_pool_2 + out_pool_3 + out_pool_4) * self.out_size\n",
    "        \n",
    "    \n",
    "    def forward(self, embedding_input, feature_input):\n",
    "        # embedding_layer\n",
    "        x = self.embedding_layer(embedding_input)\n",
    "\n",
    "        #apply convolution layers\n",
    "        x1 = self.conv_1(x)\n",
    "        x1 = torch.relu(x1)\n",
    "        x1 = self.pooling_1(x1)\n",
    "\n",
    "        x2 = self.conv_2(x)\n",
    "        x2 = torch.relu(x2)\n",
    "        x2 = self.pooling_2(x2)\n",
    "\n",
    "        x3 = self.conv_3(x)\n",
    "        x3 = torch.relu(x3)\n",
    "        x3 = self.pooling_3(x3)\n",
    "\n",
    "        x4 = self.conv_4(x)\n",
    "        x4 = torch.relu(x4)\n",
    "        x4 = self.pooling_4(x4)\n",
    "        \n",
    "        # feature layer\n",
    "        feature_layer = self.feature_layer(feature_input)\n",
    "\n",
    "        #concatenate outputs from convolutional layers\n",
    "        result = torch.cat((x1, x2, x3, x4),2)\n",
    "        result = result.reshape(result.size(0),-1)\n",
    "\n",
    "        #combine output from convolutional layers and number features\n",
    "        combined = torch.cat((result, feature_layer), 1)\n",
    "        combined_layer = self.combined_layer(combined)\n",
    "\n",
    "        #pass through fully connected layer\n",
    "        out = self.fc(combined_layer)\n",
    "        #out = self.fc(result)  \t            #without number features\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "\n",
    "        #apply activation\n",
    "        out = torch.sigmoid(out)\n",
    "        \n",
    "        # output layer\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convoNN(\n",
       "  (embedding_layer): Embedding(11401, 200)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (conv_1): Conv1d(13, 32, kernel_size=(2,), stride=(2,))\n",
       "  (conv_2): Conv1d(13, 32, kernel_size=(3,), stride=(2,))\n",
       "  (conv_3): Conv1d(13, 32, kernel_size=(4,), stride=(2,))\n",
       "  (conv_4): Conv1d(13, 32, kernel_size=(5,), stride=(2,))\n",
       "  (pooling_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pooling_2): MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pooling_3): MaxPool1d(kernel_size=4, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pooling_4): MaxPool1d(kernel_size=5, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (feature_layer): Linear(in_features=3, out_features=3, bias=True)\n",
       "  (combined_layer): Linear(in_features=6211, out_features=24, bias=True)\n",
       "  (fc): Linear(in_features=24, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialise model\n",
    "convoNN(embedding_matrix, 24, 3)\n",
    "#model = convoNN(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding\n",
    "sentences that are shorter than 13 words are padded with zeroes, sentences that are longer are truncated to length 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(tweet):\n",
    "    temp_tweet = list(tweet.split(\" \"))\n",
    "    if len(temp_tweet) == 1:\n",
    "        for x in range(14):\n",
    "            temp_tweet.append(\"0\")\n",
    "        tweet = \" \".join(tweet)\n",
    "    if len(temp_tweet) < 13:\n",
    "        x = 13 - len(temp_tweet)\n",
    "        for x in range(x):\n",
    "            temp_tweet.append(\"0\")\n",
    "        tweet = \" \".join(temp_tweet)\n",
    "        return tweet\n",
    "    if len(temp_tweet) > 13:\n",
    "        y = len(temp_tweet) - 13\n",
    "        tweet = temp_tweet[y:]\n",
    "        tweet = \" \".join(tweet)\n",
    "        return tweet\n",
    "    else:\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load text\n",
    "train_samples_prepad = dtf[\"text\"].to_list()\n",
    "print(len(train_samples_prepad))\n",
    "\n",
    "#apply padding to training samples\n",
    "x_train = []\n",
    "for x in train_samples_prepad:\n",
    "    new = pad(x)\n",
    "    x_train.append(new)\n",
    "\n",
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#load features\n",
    "train_caps = dtf[\"caps\"].to_list()\n",
    "train_sentiment = dtf[\"sntmt\"].to_list()\n",
    "train_nohs = dtf[\"nohs\"].to_list()\n",
    "\n",
    "#check if everything same length\n",
    "print(len(train_caps) == len(train_sentiment) == len(train_nohs))\n",
    "\n",
    "#make lists of features into array\n",
    "train_features = pd.DataFrame(train_caps)\n",
    "train_features.columns = [\"caps\"]\n",
    "train_features[\"sentiment\"] = train_sentiment\n",
    "train_features[\"nohs\"] = train_nohs\n",
    "train_features_arr = train_features.to_numpy()\n",
    "train_features_arr = torch.FloatTensor(train_features_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load labels\n",
    "#HS TR AG\n",
    "train_label = dtf[\"HS\"].to_list()    #hate speech\n",
    "train_label_tr = dtf[\"TR\"].to_list()    #targeted\n",
    "train_label_ag = dtf[\"AG\"].to_list()    #aggressive\n",
    "\n",
    "#for subtask B: select required label\n",
    "#train_label = train_label_tr\n",
    "#train_label = train_label_ag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load text from trainibg\n",
    "test_samples_prepad = dtf_test[\"text\"].to_list()\n",
    "\n",
    "#pad test sentences\n",
    "x_test = []\n",
    "for x in test_samples_prepad:\n",
    "    new = pad(x)\n",
    "    x_test.append(new)\n",
    "\n",
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#load features\n",
    "test_caps = dtf_test[\"caps\"].to_list()\n",
    "test_sentiment = dtf_test[\"sntmt\"].to_list()\n",
    "test_nohs = dtf_test[\"nohs\"].to_list()\n",
    "\n",
    "#check if everything same length\n",
    "print(len(test_caps) == len(test_sentiment) == len(test_nohs))\n",
    "\n",
    "#make lists of features into dataframe\n",
    "test_features = pd.DataFrame(test_caps)\n",
    "test_features.columns = [\"caps\"]\n",
    "test_features[\"sentiment\"] = test_sentiment\n",
    "test_features[\"nohs\"] = test_nohs\n",
    "test_features_arr = test_features.to_numpy()\n",
    "test_features_arr = torch.FloatTensor(test_features_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = dtf_test[\"HS\"].to_list()\n",
    "test_label_tr = dtf_test[\"TR\"].to_list()\n",
    "test_label_ag = dtf_test[\"AG\"].to_list()\n",
    "\n",
    "#for subtask B: select required label\n",
    "#test_label = test_label_tr\n",
    "#test_label = test_label_ag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get IDs per word - input for embedding layer of model\n",
    "def get_ids_from_words(samples, assignment_dict):\n",
    "    final_ids = []\n",
    "    for x in samples:\n",
    "        temp = x.split()\n",
    "        ids = []\n",
    "        for sample in temp:\n",
    "            if sample in assignment_dict.keys():\n",
    "                ids.append(assignment_dict[sample])\n",
    "            else:\n",
    "                ids.append(0)\n",
    "        final_ids.append(ids)\n",
    "    return torch.tensor(final_ids, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = get_ids_from_words(x_train, tokenizer.word_index)\n",
    "test_ids = get_ids_from_words(x_test, tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetMapper(Dataset):\n",
    "    def __init__(self, tweet, features, label):\n",
    "        self.tweet = tweet            \n",
    "        self.features = features            \n",
    "        self.label = label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tweet)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return  self.tweet[idx], self.features[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DatasetMapper(train_ids, train_features_arr, train_label)\n",
    "test = DatasetMapper(test_ids, test_features_arr, test_label)\n",
    "loader_train = DataLoader(train, batch_size=32)\n",
    "loader_test = DataLoader(test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_ids, train_features_arr, train_label, test_ids, test_features_arr, test_label):\n",
    "    \n",
    "    train = DatasetMapper(train_ids, train_features_arr, train_label)\n",
    "    test = DatasetMapper(test_ids, test_features_arr, test_label)   \n",
    "   \n",
    "    # Initialize loaders\n",
    "    loader_train = DataLoader(train, batch_size=32)\n",
    "    loader_test = DataLoader(test, batch_size=32)\n",
    "   \n",
    "   #set learning rate and select optimiser\n",
    "    learning_rate = 0.001\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "   \n",
    "    # Starts training phase\n",
    "    for epoch in range(100):\n",
    "        # Set model in training model\n",
    "        model.train()\n",
    "        predictions = []\n",
    "        # Starts batch training\n",
    "        for tweet_batch, features_batch, label_batch in loader_train:\n",
    "      \n",
    "            label_batch = label_batch.type(torch.FloatTensor)\n",
    "         \n",
    "            # Feed the model\n",
    "            label_pred = model(tweet_batch, features_batch)\n",
    "         \n",
    "            # Loss calculation\n",
    "            loss = F.binary_cross_entropy(label_pred, label_batch)\n",
    "         \n",
    "            optimizer.zero_grad()\n",
    "         \n",
    "            # backwards pass\n",
    "            loss.backward()\n",
    "         \n",
    "            # Gradients update\n",
    "            optimizer.step()\n",
    "         \n",
    "            # Save predictions\n",
    "            predictions += list(label_pred.detach().numpy())\n",
    "      \n",
    "        # Metrics calculation\n",
    "        train_accuary = accuracy_score(train_label, np.around(predictions))\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(\"Epoch: %d, loss: %.5f, Train accuracy: %.5f\" % (epoch+1, loss.item(), train_accuary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model instance\n",
    "model = convoNN(embedding_matrix, 24, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n",
    "train(model, train_ids, train_features_arr, train_label, test_ids, test_features_arr, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predicted labels for the test set\n",
    "pred_labels = model(test_ids, test_features_arr)\n",
    "predictions_test_binary = list()\n",
    "\n",
    "#round values: =<0.5 --> 0, > 0.5 --> 1\n",
    "for x in pred_labels:\n",
    "    y = torch.round(x)\n",
    "    y = y.item()\n",
    "    predictions_test_binary.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate performance of model on subtask A\n",
    "def evaluate_a(predictions_test,gold_data):\n",
    "    levels = [\"HS\"]\n",
    "    ground_truth = gold_data\n",
    "\n",
    "    predicted = predictions_test\n",
    "    ground_truth[\"predicted\"] = predicted\n",
    "\n",
    "    # Check length files\n",
    "    if (len(ground_truth) != len(predicted)):\n",
    "        sys.exit('Prediction and gold data have different number of lines.')\n",
    "\n",
    "    # Check predicted classes\n",
    "    for c in levels:\n",
    "        gt_class = list(ground_truth[c].value_counts().keys())\n",
    "        for value in predicted:\n",
    "            if not value in gt_class:\n",
    "                sys.exit(\"Wrong value in \" + c + \" prediction column.\")\n",
    "\n",
    "    # Compute Performance Measures HS\n",
    "    acc_hs = accuracy_score(ground_truth[\"HS\"], ground_truth[\"predicted\"])\n",
    "    [p_nohs, p_hs], [r_nohs, r_hs], [f1_nohs, f1_hs], support = precision_recall_fscore_support(ground_truth[\"HS\"], ground_truth[\"predicted\"], pos_label = 1)\n",
    "    p_macro, r_macro, f1_macro, support = precision_recall_fscore_support(ground_truth[\"HS\"], ground_truth[\"predicted\"], average = \"macro\")\n",
    "\n",
    "    return acc_hs, p_hs, p_nohs, r_hs, r_nohs, f1_hs, f1_nohs, p_macro, r_macro, f1_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate performance of model on subtask b\n",
    "def evaluate_b(pred,gold):\n",
    "    levels = [\"HS\", \"TR\", \"AG\"]\n",
    "\n",
    "    ground_truth = gold\n",
    "    predicted = pred\n",
    "\n",
    "    # Check length files\n",
    "    if (len(ground_truth) != len(predicted)):\n",
    "        sys.exit('Prediction and gold data have different number of lines.')\n",
    "\n",
    "    # Check predicted classes\n",
    "    for c in levels:\n",
    "        gt_class = list(ground_truth[c].value_counts().keys())\n",
    "        if not (predicted[c].isin(gt_class).all()):\n",
    "            sys.exit(\"Wrong value in \" + c + \" prediction column.\")\n",
    "\n",
    "    data = pd.merge(ground_truth, predicted, on=\"id\")\n",
    "\n",
    "    if (len(ground_truth) != len(data)):\n",
    "        sys.exit('Invalid tweet IDs in prediction.')\n",
    "\n",
    "    # Compute Performance Measures\n",
    "    acc_levels = dict.fromkeys(levels)\n",
    "    p_levels = dict.fromkeys(levels)\n",
    "    r_levels = dict.fromkeys(levels)\n",
    "    f1_levels = dict.fromkeys(levels)\n",
    "    for l in levels:\n",
    "        acc_levels[l] = accuracy_score(data[l + \"_x\"], data[l + \"_y\"])\n",
    "        p_levels[l], r_levels[l], f1_levels[l], _ = precision_recall_fscore_support(data[l + \"_x\"], data[l + \"_y\"], average=\"macro\")\n",
    "    macro_f1 = np.mean(list(f1_levels.values()))\n",
    "\n",
    "    # Compute Exact Match Ratio\n",
    "    check_emr = np.ones(len(data), dtype=bool)\n",
    "    for l in levels:\n",
    "        check_label = data[l + \"_x\"] == data[l + \"_y\"]\n",
    "        check_emr = check_emr & check_label\n",
    "    emr = sum(check_emr) / len(data)\n",
    "\n",
    "    return macro_f1, emr, acc_levels, p_levels, r_levels, f1_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print evaluation A\n",
    "acc_hs, p_hs, p_nohs, r_hs, r_nohs, f1_hs, f1_nohs, p_macro, r_macro, f1_macro = evaluate_a(predictions_test_binary, dtf_test)\n",
    "\n",
    "print(\"\\t\".join([\"{}\".format(x) for x in [\"acc.\", \"P (1)\", \"P (0)\", \"R (1)\", \"R (0)\", \"F1 (1)\", \"F1 (0)\", \"P (avg)\", \"R (avg)\", \"F1 (avg)\"]]))\n",
    "print(\"\\t\".join([\"{0:.3f}\".format(x) for x in [acc_hs, p_hs, p_nohs, r_hs, r_nohs, f1_hs, f1_nohs, p_macro, r_macro, f1_macro]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = r\"es_model.sav\"\n",
    "# pickle.dump(model, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unpickle model and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(r\"es_model.sav\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = loaded_model(test_ids, test_features_arr)\n",
    "predictions_test_binary_x = list()\n",
    "\n",
    "for x in predicted_labels:\n",
    "    y = torch.round(x)\n",
    "    y = y.item()\n",
    "    predictions_test_binary_x.append(y)\n",
    "\n",
    "# for subtask B: all dimensions must be saved to be evaluated using evaluation B - for individual evaluation of dimensions use evaluation A\n",
    "#tr = predictions_test_binary_x\n",
    "#ag = predictions_test_binary_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dtf = dtf_test\n",
    "pred_dtf = pred_dtf.drop(columns=[\"TR\", \"AG\"], axis = 1)\n",
    "\n",
    "pred_dtf[\"TR\"] = tr\n",
    "pred_dtf[\"AG\"] = ag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc.\tP (1)\tP (0)\tR (1)\tR (0)\tF1 (1)\tF1 (0)\tP (avg)\tR (avg)\tF1 (avg)\n",
      "0.555\t0.478\t0.657\t0.649\t0.487\t0.551\t0.560\t0.568\t0.568\t0.555\n"
     ]
    }
   ],
   "source": [
    "#SUBTASK A EVAL\n",
    "\n",
    "acc_hs, p_hs, p_nohs, r_hs, r_nohs, f1_hs, f1_nohs, p_macro, r_macro, f1_macro = evaluate_a(predictions_test_binary_x, dtf_test)\n",
    "\n",
    "print(\"\\t\".join([\"{}\".format(x) for x in [\"acc.\", \"P (1)\", \"P (0)\", \"R (1)\", \"R (0)\", \"F1 (1)\", \"F1 (0)\", \"P (avg)\", \"R (avg)\", \"F1 (avg)\"]]))\n",
    "print(\"\\t\".join([\"{0:.3f}\".format(x) for x in [acc_hs, p_hs, p_nohs, r_hs, r_nohs, f1_hs, f1_nohs, p_macro, r_macro, f1_macro]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUBTASK B EVAL\n",
    "macro_f1, emr, acc_levels, p_levels, r_levels, f1_levels = evaluate_b(pred_dtf, dtf_test)\n",
    "\n",
    "print(\"\\t\".join([\"{}\".format(x) for x in [\"acc_HS\", \"acc_TR\", \"acc_AG\", \"p_HS\", \"p_TR\", \"p_AG\", \"r_HS\", \"r_TR\", \"r_AG\", \"f1_HS\", \"f1_TR\", \"f1_AG\", \"emr\", \"macro_f1\"]]))\n",
    "print(\"\\t\".join([\"{0:.3f}\".format(x) for x in [acc_levels[\"HS\"], acc_levels[\"TR\"], acc_levels[\"AG\"], p_levels[\"HS\"], p_levels[\"TR\"], p_levels[\"AG\"], r_levels[\"HS\"], r_levels[\"TR\"], r_levels[\"AG\"], f1_levels[\"HS\"], f1_levels[\"TR\"], f1_levels[\"AG\"], emr, macro_f1]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('cosi114a')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8b0c4d5a664346d8579ecc97193be4c7a99fd655324ca52e586a554ecdcce75"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
