{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\University\\\\Bachelorarbeit\\\\model\\\\hateval2019\\\\english\\\\en_train_normalised.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-76968c20a522>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#import text data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#english - subtask a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdtf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"D:\\University\\Bachelorarbeit\\model\\hateval2019\\english\\en_train_normalised.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_filter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m             \u001b[1;31m#training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdtf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"D:\\University\\Bachelorarbeit\\model\\hateval2019\\english\\en_test_normalised.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_filter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m         \u001b[1;31m#testing data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#dtf_test = pd.read_csv(r\"D:\\University\\Bachelorarbeit\\model\\hateval2019\\english\\en_dev_normalised.csv\", delimiter=\",\", na_filter=False, encoding=\"utf-8\")         #development\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dlnlp\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dlnlp\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dlnlp\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dlnlp\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dlnlp\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dlnlp\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dlnlp\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\University\\\\Bachelorarbeit\\\\model\\\\hateval2019\\\\english\\\\en_train_normalised.csv'"
     ]
    }
   ],
   "source": [
    "#import text data\n",
    "#english - subtask a\n",
    "dtf = pd.read_csv(r\"D:\\University\\Bachelorarbeit\\model\\hateval2019\\english\\en_train_normalised.csv\", delimiter=\",\", na_filter=False, encoding=\"utf-8\")             #training data\n",
    "dtf_test = pd.read_csv(r\"D:\\University\\Bachelorarbeit\\model\\hateval2019\\english\\en_test_normalised.csv\", delimiter=\",\", na_filter=False, encoding=\"utf-8\")         #testing data\n",
    "#dtf_test = pd.read_csv(r\"D:\\University\\Bachelorarbeit\\model\\hateval2019\\english\\en_dev_normalised.csv\", delimiter=\",\", na_filter=False, encoding=\"utf-8\")         #development \n",
    "\n",
    "#subtask b\n",
    "#dtf = pd.read_csv(r\"D:\\University\\Bachelorarbeit\\model\\hateval2019\\english\\subtask_b\\b_en_train.csv\", delimiter=\",\", na_filter=False, encoding=\"utf-8\")\n",
    "#dtf_test = pd.read_csv(r\"D:\\University\\Bachelorarbeit\\model\\hateval2019\\english\\subtask_b\\b_en_test.csv\", delimiter=\",\", na_filter=False, encoding=\"utf-8\")\n",
    "\n",
    "#spanish text\n",
    "#dtf = pd.read_csv(r\"D:\\University\\Bachelorarbeit\\model\\hateval2019\\spanish\\es_train_normalised.csv\", delimiter=\",\", na_filter=False, encoding=\"utf-8\")\n",
    "#dtf_test = pd.read_csv(r\"D:\\University\\Bachelorarbeit\\model\\hateval2019\\spanish\\es_test_normalised.csv\", delimiter=\",\", na_filter=False, encoding=\"utf-8\")\n",
    "\n",
    "#subtask b\n",
    "#dtf = pd.read_csv(r\"D:\\University\\Bachelorarbeit\\model\\hateval2019\\spanish\\subtask_b\\b_es_train.csv\", delimiter=\",\", na_filter=False, encoding=\"utf-8\")\n",
    "#dtf_test = pd.read_csv(r\"D:\\University\\Bachelorarbeit\\model\\hateval2019\\spanish\\subtask_b\\b_es_test.csv\", delimiter=\",\", na_filter=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the glove embeddings\n",
    "path  =r\"D:\\University\\Bachelorarbeit\\model\\glovevectors\\glove.twitter.27B.200d.txt\"\n",
    "\n",
    "glove = pd.read_csv(path, sep=\" \", quoting=3, header=None, index_col=0)\n",
    "glove_embedding = {key: val.values for key, val in glove.T.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create embedding matrix - look-table for embeddings based on index\n",
    "def create_embedding_matrix(word_index,embedding_dict,dimension):\n",
    "  embedding_matrix=np.zeros((len(word_index)+1,dimension))      #maybe better to use rand??\n",
    " \n",
    "  for word,index in word_index.items():\n",
    "    if word in embedding_dict:\n",
    "      embedding_matrix[index]=embedding_dict[word]\n",
    "  return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the embedding matrix on the testing vocabulary\n",
    "text = dtf[\"text\"].tolist()\n",
    " \n",
    "tokenizer=tf.keras.preprocessing.text.Tokenizer(split=\" \")\n",
    "tokenizer.fit_on_texts(text)\n",
    " \n",
    "text_token=tokenizer.texts_to_sequences(text)\n",
    " \n",
    "embedding_matrix = create_embedding_matrix(tokenizer.word_index,embedding_dict=glove_embedding,dimension=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = embedding_matrix.shape[0]              #no. words in dataset --> 11400\n",
    "vector_size = embedding_matrix.shape[1]             #dimension of vectors --> 200\n",
    "\n",
    "embedding = nn.Embedding(num_embeddings=vocab_size,embedding_dim=vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise embedding layer using pre-trained weights\n",
    "embedding.weight=nn.Parameter(torch.tensor(embedding_matrix,dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convoNN(nn.ModuleList):\n",
    "    #def __init__(self, weights_matrix, hidden_size, number_feature):\n",
    "    def __init__(self, weights_matrix, hidden_size, number_feature):\n",
    "        super(convoNN, self).__init__()\n",
    "        # intialize embedding layer with required size\n",
    "        self.vocab_size = weights_matrix.shape[0]              #no. words in dataset\n",
    "        self.vector_size = weights_matrix.shape[1]             #dimension of vectors\n",
    "\n",
    "        #embedding\n",
    "        self.embedding_layer = nn.Embedding(self.vocab_size + 1, self.vector_size)\n",
    "        #initialise embedding layer using pre-trained weights\n",
    "        self.embedding_layer.weight=nn.Parameter(torch.tensor(weights_matrix,dtype=torch.float32))\n",
    "        #disable learning bc pre-trained\n",
    "        #self.embedding_layer.weight.requires_grad=False\n",
    "\n",
    "        self.stride = 2\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.seq_len = 13\n",
    "        self.out_size = 32\n",
    "\n",
    "        #kernels\n",
    "        self.kernel_1 = 2\n",
    "        self.kernel_2 = 3\n",
    "        self.kernel_3 = 4\n",
    "        self.kernel_4 = 5\n",
    "\n",
    "        #convolution layers\n",
    "        self.conv_1 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_1, self.stride)     \n",
    "        self.conv_2 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_2, self.stride)\n",
    "        self.conv_3 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_3, self.stride)\n",
    "        self.conv_4 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_4, self.stride)\n",
    "\n",
    "        #pooling layers\n",
    "        self.pooling_1 = nn.MaxPool1d(self.kernel_1, self.stride)\n",
    "        self.pooling_2 = nn.MaxPool1d(self.kernel_2, self.stride)\n",
    "        self.pooling_3 = nn.MaxPool1d(self.kernel_3, self.stride)\n",
    "        self.pooling_4 = nn.MaxPool1d(self.kernel_4, self.stride)\n",
    "\n",
    "        #feature layer\n",
    "        self.feature_layer = nn.Linear(number_feature, number_feature).float()\n",
    "\n",
    "        # combination layer\n",
    "        self.combined_layer = nn.Linear(self.in_features_fc()+number_feature, hidden_size).float()        \n",
    "        \n",
    "    \t#fully connected layer\n",
    "        #self.fc = nn.Linear(self.in_features_fc(), 1)              #without features\n",
    "        self.fc = nn.Linear(hidden_size, 1)                         #with features\n",
    "    \n",
    "    def in_features_fc(self):\n",
    "\n",
    "      self.embedding_size = 200\n",
    "      # Calcualte size of convolved/pooled features for convolution_1/max_pooling_1 features\n",
    "      out_conv_1 = ((self.embedding_size - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n",
    "      out_conv_1 = math.floor(out_conv_1)\n",
    "      out_pool_1 = ((out_conv_1 - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n",
    "      out_pool_1 = math.floor(out_pool_1)\n",
    "      \n",
    "      # Calcualte size of convolved/pooled features for convolution_2/max_pooling_2 features\n",
    "      out_conv_2 = ((self.embedding_size - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n",
    "      out_conv_2 = math.floor(out_conv_2)\n",
    "      out_pool_2 = ((out_conv_2 - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n",
    "      out_pool_2 = math.floor(out_pool_2)\n",
    "      \n",
    "      # Calcualte size of convolved/pooled features for convolution_3/max_pooling_3 features\n",
    "      out_conv_3 = ((self.embedding_size - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n",
    "      out_conv_3 = math.floor(out_conv_3)\n",
    "      out_pool_3 = ((out_conv_3 - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n",
    "      out_pool_3 = math.floor(out_pool_3)\n",
    "      \n",
    "      # Calcualte size of convolved/pooled features for convolution_4/max_pooling_4 features\n",
    "      out_conv_4 = ((self.embedding_size - 1 * (self.kernel_4 - 1) - 1) / self.stride) + 1\n",
    "      out_conv_4 = math.floor(out_conv_4)\n",
    "      out_pool_4 = ((out_conv_4 - 1 * (self.kernel_4 - 1) - 1) / self.stride) + 1\n",
    "      out_pool_4 = math.floor(out_pool_4)\n",
    "      \n",
    "      # Returns \"flattened\" vector (input for fully connected layer)\n",
    "      return (out_pool_1 + out_pool_2 + out_pool_3 + out_pool_4) * self.out_size\n",
    "        \n",
    "    \n",
    "    def forward(self, embedding_input, feature_input):\n",
    "        # embedding_layer\n",
    "        x = self.embedding_layer(embedding_input)\n",
    "\n",
    "        #apply convolution layers\n",
    "        x1 = self.conv_1(x)\n",
    "        x1 = torch.relu(x1)\n",
    "        x1 = self.pooling_1(x1)\n",
    "\n",
    "        x2 = self.conv_2(x)\n",
    "        x2 = torch.relu(x2)\n",
    "        x2 = self.pooling_2(x2)\n",
    "\n",
    "        x3 = self.conv_3(x)\n",
    "        x3 = torch.relu(x3)\n",
    "        x3 = self.pooling_3(x3)\n",
    "\n",
    "        x4 = self.conv_4(x)\n",
    "        x4 = torch.relu(x4)\n",
    "        x4 = self.pooling_4(x4)\n",
    "        \n",
    "        # feature layer\n",
    "        feature_layer = self.feature_layer(feature_input)\n",
    "\n",
    "        #concatenate outputs from convolutional layers\n",
    "        result = torch.cat((x1, x2, x3, x4),2)\n",
    "        result = result.reshape(result.size(0),-1)\n",
    "\n",
    "        #combine output from convolutional layers and number features\n",
    "        combined = torch.cat((result, feature_layer), 1)\n",
    "        combined_layer = self.combined_layer(combined)\n",
    "\n",
    "        #pass through fully connected layer\n",
    "        out = self.fc(combined_layer)\n",
    "        #out = self.fc(result)  \t            #without number features\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "\n",
    "        #apply activation\n",
    "        out = torch.sigmoid(out)\n",
    "        \n",
    "        # output layer\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convoNN(\n",
       "  (embedding_layer): Embedding(11401, 200)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (conv_1): Conv1d(13, 32, kernel_size=(2,), stride=(2,))\n",
       "  (conv_2): Conv1d(13, 32, kernel_size=(3,), stride=(2,))\n",
       "  (conv_3): Conv1d(13, 32, kernel_size=(4,), stride=(2,))\n",
       "  (conv_4): Conv1d(13, 32, kernel_size=(5,), stride=(2,))\n",
       "  (pooling_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pooling_2): MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pooling_3): MaxPool1d(kernel_size=4, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pooling_4): MaxPool1d(kernel_size=5, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (feature_layer): Linear(in_features=3, out_features=3, bias=True)\n",
       "  (combined_layer): Linear(in_features=6211, out_features=24, bias=True)\n",
       "  (fc): Linear(in_features=24, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialise model\n",
    "convoNN(embedding_matrix, 24, 3)\n",
    "#model = convoNN(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding\n",
    "sentences that are shorter than 13 words are padded with zeroes, sentences that are longer are truncated to length 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(tweet):\n",
    "    temp_tweet = list(tweet.split(\" \"))\n",
    "    if len(temp_tweet) == 1:\n",
    "        for x in range(14):\n",
    "            temp_tweet.append(\"0\")\n",
    "        tweet = \" \".join(tweet)\n",
    "    if len(temp_tweet) < 13:\n",
    "        x = 13 - len(temp_tweet)\n",
    "        for x in range(x):\n",
    "            temp_tweet.append(\"0\")\n",
    "        tweet = \" \".join(temp_tweet)\n",
    "        return tweet\n",
    "    if len(temp_tweet) > 13:\n",
    "        y = len(temp_tweet) - 13\n",
    "        tweet = temp_tweet[y:]\n",
    "        tweet = \" \".join(tweet)\n",
    "        return tweet\n",
    "    else:\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load text\n",
    "train_samples_prepad = dtf[\"text\"].to_list()\n",
    "print(len(train_samples_prepad))\n",
    "\n",
    "#apply padding to training samples\n",
    "x_train = []\n",
    "for x in train_samples_prepad:\n",
    "    new = pad(x)\n",
    "    x_train.append(new)\n",
    "\n",
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#load features\n",
    "train_caps = dtf[\"caps\"].to_list()\n",
    "train_sentiment = dtf[\"sntmt\"].to_list()\n",
    "train_nohs = dtf[\"nohs\"].to_list()\n",
    "\n",
    "#check if everything same length\n",
    "print(len(train_caps) == len(train_sentiment) == len(train_nohs))\n",
    "\n",
    "#make lists of features into array\n",
    "train_features = pd.DataFrame(train_caps)\n",
    "train_features.columns = [\"caps\"]\n",
    "train_features[\"sentiment\"] = train_sentiment\n",
    "train_features[\"nohs\"] = train_nohs\n",
    "train_features_arr = train_features.to_numpy()\n",
    "train_features_arr = torch.FloatTensor(train_features_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load labels\n",
    "#HS TR AG\n",
    "train_label = dtf[\"HS\"].to_list()    #hate speech\n",
    "train_label_tr = dtf[\"TR\"].to_list()    #targeted\n",
    "train_label_ag = dtf[\"AG\"].to_list()    #aggressive\n",
    "\n",
    "#for subtask B: select required label\n",
    "#train_label = train_label_tr\n",
    "#train_label = train_label_ag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load text from trainibg\n",
    "test_samples_prepad = dtf_test[\"text\"].to_list()\n",
    "\n",
    "#pad test sentences\n",
    "x_test = []\n",
    "for x in test_samples_prepad:\n",
    "    new = pad(x)\n",
    "    x_test.append(new)\n",
    "\n",
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#load features\n",
    "test_caps = dtf_test[\"caps\"].to_list()\n",
    "test_sentiment = dtf_test[\"sntmt\"].to_list()\n",
    "test_nohs = dtf_test[\"nohs\"].to_list()\n",
    "\n",
    "#check if everything same length\n",
    "print(len(test_caps) == len(test_sentiment) == len(test_nohs))\n",
    "\n",
    "#make lists of features into dataframe\n",
    "test_features = pd.DataFrame(test_caps)\n",
    "test_features.columns = [\"caps\"]\n",
    "test_features[\"sentiment\"] = test_sentiment\n",
    "test_features[\"nohs\"] = test_nohs\n",
    "test_features_arr = test_features.to_numpy()\n",
    "test_features_arr = torch.FloatTensor(test_features_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = dtf_test[\"HS\"].to_list()\n",
    "test_label_tr = dtf_test[\"TR\"].to_list()\n",
    "test_label_ag = dtf_test[\"AG\"].to_list()\n",
    "\n",
    "#for subtask B: select required label\n",
    "#test_label = test_label_tr\n",
    "#test_label = test_label_ag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get IDs per word - input for embedding layer of model\n",
    "def get_ids_from_words(samples, assignment_dict):\n",
    "    final_ids = []\n",
    "    for x in samples:\n",
    "        temp = x.split()\n",
    "        ids = []\n",
    "        for sample in temp:\n",
    "            if sample in assignment_dict.keys():\n",
    "                ids.append(assignment_dict[sample])\n",
    "            else:\n",
    "                ids.append(0)\n",
    "        final_ids.append(ids)\n",
    "    return torch.tensor(final_ids, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = get_ids_from_words(x_train, tokenizer.word_index)\n",
    "test_ids = get_ids_from_words(x_test, tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetMapper(Dataset):\n",
    "    def __init__(self, tweet, features, label):\n",
    "        self.tweet = tweet            \n",
    "        self.features = features            \n",
    "        self.label = label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tweet)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return  self.tweet[idx], self.features[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DatasetMapper(train_ids, train_features_arr, train_label)\n",
    "test = DatasetMapper(test_ids, test_features_arr, test_label)\n",
    "loader_train = DataLoader(train, batch_size=32)\n",
    "loader_test = DataLoader(test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_ids, train_features_arr, train_label, test_ids, test_features_arr, test_label):\n",
    "    \n",
    "    train = DatasetMapper(train_ids, train_features_arr, train_label)\n",
    "    test = DatasetMapper(test_ids, test_features_arr, test_label)   \n",
    "   \n",
    "    # Initialize loaders\n",
    "    loader_train = DataLoader(train, batch_size=32)\n",
    "    loader_test = DataLoader(test, batch_size=32)\n",
    "   \n",
    "   #set learning rate and select optimiser\n",
    "    learning_rate = 0.001\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "   \n",
    "    # Starts training phase\n",
    "    for epoch in range(100):\n",
    "        # Set model in training model\n",
    "        model.train()\n",
    "        predictions = []\n",
    "        # Starts batch training\n",
    "        for tweet_batch, features_batch, label_batch in loader_train:\n",
    "      \n",
    "            label_batch = label_batch.type(torch.FloatTensor)\n",
    "         \n",
    "            # Feed the model\n",
    "            label_pred = model(tweet_batch, features_batch)\n",
    "         \n",
    "            # Loss calculation\n",
    "            loss = F.binary_cross_entropy(label_pred, label_batch)\n",
    "         \n",
    "            optimizer.zero_grad()\n",
    "         \n",
    "            # backwards pass\n",
    "            loss.backward()\n",
    "         \n",
    "            # Gradients update\n",
    "            optimizer.step()\n",
    "         \n",
    "            # Save predictions\n",
    "            predictions += list(label_pred.detach().numpy())\n",
    "      \n",
    "        # Metrics calculation\n",
    "        train_accuary = accuracy_score(train_label, np.around(predictions))\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(\"Epoch: %d, loss: %.5f, Train accuracy: %.5f\" % (epoch+1, loss.item(), train_accuary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model instance\n",
    "model = convoNN(embedding_matrix, 24, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 0.56929, Train accuracy: 0.67978\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-a0ab7f213e52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_features_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_features_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-34-d46d5726d040>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_ids, train_features_arr, train_label, test_ids, test_features_arr, test_label)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;31m# Gradients update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;31m# Save predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dlnlp\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dlnlp\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dlnlp\\lib\\site-packages\\torch\\optim\\rmsprop.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             F.rmsprop(params_with_grad,\n\u001b[0m\u001b[0;32m    107\u001b[0m                       \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m                       \u001b[0msquare_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dlnlp\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36mrmsprop\u001b[1;34m(params, grads, square_avgs, grad_avgs, momentum_buffer_list, lr, alpha, eps, weight_decay, momentum, centered)\u001b[0m\n\u001b[0;32m    237\u001b[0m             \u001b[0mavg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msquare_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m             \u001b[0mavg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msquare_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train model\n",
    "train(model, train_ids, train_features_arr, train_label, test_ids, test_features_arr, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predicted labels for the test set\n",
    "pred_labels = model(test_ids, test_features_arr)\n",
    "predictions_test_binary = list()\n",
    "\n",
    "#round values: =<0.5 --> 0, > 0.5 --> 1\n",
    "for x in pred_labels:\n",
    "    y = torch.round(x)\n",
    "    y = y.item()\n",
    "    predictions_test_binary.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate performance of model on subtask A\n",
    "def evaluate_a(predictions_test,gold_data):\n",
    "    levels = [\"HS\"]\n",
    "    ground_truth = gold_data\n",
    "\n",
    "    predicted = predictions_test\n",
    "    ground_truth[\"predicted\"] = predicted\n",
    "\n",
    "    # Check length files\n",
    "    if (len(ground_truth) != len(predicted)):\n",
    "        sys.exit('Prediction and gold data have different number of lines.')\n",
    "\n",
    "    # Check predicted classes\n",
    "    for c in levels:\n",
    "        gt_class = list(ground_truth[c].value_counts().keys())\n",
    "        for value in predicted:\n",
    "            if not value in gt_class:\n",
    "                sys.exit(\"Wrong value in \" + c + \" prediction column.\")\n",
    "\n",
    "    # Compute Performance Measures HS\n",
    "    acc_hs = accuracy_score(ground_truth[\"HS\"], ground_truth[\"predicted\"])\n",
    "    [p_nohs, p_hs], [r_nohs, r_hs], [f1_nohs, f1_hs], support = precision_recall_fscore_support(ground_truth[\"HS\"], ground_truth[\"predicted\"], pos_label = 1)\n",
    "    p_macro, r_macro, f1_macro, support = precision_recall_fscore_support(ground_truth[\"HS\"], ground_truth[\"predicted\"], average = \"macro\")\n",
    "\n",
    "    return acc_hs, p_hs, p_nohs, r_hs, r_nohs, f1_hs, f1_nohs, p_macro, r_macro, f1_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate performance of model on subtask b\n",
    "def evaluate_b(pred,gold):\n",
    "    levels = [\"HS\", \"TR\", \"AG\"]\n",
    "\n",
    "    ground_truth = gold\n",
    "    predicted = pred\n",
    "\n",
    "    # Check length files\n",
    "    if (len(ground_truth) != len(predicted)):\n",
    "        sys.exit('Prediction and gold data have different number of lines.')\n",
    "\n",
    "    # Check predicted classes\n",
    "    for c in levels:\n",
    "        gt_class = list(ground_truth[c].value_counts().keys())\n",
    "        if not (predicted[c].isin(gt_class).all()):\n",
    "            sys.exit(\"Wrong value in \" + c + \" prediction column.\")\n",
    "\n",
    "    data = pd.merge(ground_truth, predicted, on=\"id\")\n",
    "\n",
    "    if (len(ground_truth) != len(data)):\n",
    "        sys.exit('Invalid tweet IDs in prediction.')\n",
    "\n",
    "    # Compute Performance Measures\n",
    "    acc_levels = dict.fromkeys(levels)\n",
    "    p_levels = dict.fromkeys(levels)\n",
    "    r_levels = dict.fromkeys(levels)\n",
    "    f1_levels = dict.fromkeys(levels)\n",
    "    for l in levels:\n",
    "        acc_levels[l] = accuracy_score(data[l + \"_x\"], data[l + \"_y\"])\n",
    "        p_levels[l], r_levels[l], f1_levels[l], _ = precision_recall_fscore_support(data[l + \"_x\"], data[l + \"_y\"], average=\"macro\")\n",
    "    macro_f1 = np.mean(list(f1_levels.values()))\n",
    "\n",
    "    # Compute Exact Match Ratio\n",
    "    check_emr = np.ones(len(data), dtype=bool)\n",
    "    for l in levels:\n",
    "        check_label = data[l + \"_x\"] == data[l + \"_y\"]\n",
    "        check_emr = check_emr & check_label\n",
    "    emr = sum(check_emr) / len(data)\n",
    "\n",
    "    return macro_f1, emr, acc_levels, p_levels, r_levels, f1_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Wrong value in HS prediction column.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m Wrong value in HS prediction column.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mayak\\anaconda3\\envs\\dlnlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3449: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#print evaluation A\n",
    "acc_hs, p_hs, p_nohs, r_hs, r_nohs, f1_hs, f1_nohs, p_macro, r_macro, f1_macro = evaluate_a(predictions_test_binary, dtf_test)\n",
    "\n",
    "print(\"\\t\".join([\"{}\".format(x) for x in [\"acc.\", \"P (1)\", \"P (0)\", \"R (1)\", \"R (0)\", \"F1 (1)\", \"F1 (0)\", \"P (avg)\", \"R (avg)\", \"F1 (avg)\"]]))\n",
    "print(\"\\t\".join([\"{0:.3f}\".format(x) for x in [acc_hs, p_hs, p_nohs, r_hs, r_nohs, f1_hs, f1_nohs, p_macro, r_macro, f1_macro]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = r\"D:\\University\\Bachelorarbeit\\model\\CNN\\es_model_ag.sav\"\n",
    "# pickle.dump(model, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unpickle model and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(r\"D:\\University\\Bachelorarbeit\\model\\CNN\\models\\en_hs_model.sav\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = loaded_model(test_ids, test_features_arr)\n",
    "predictions_test_binary_x = list()\n",
    "\n",
    "for x in predicted_labels:\n",
    "    y = torch.round(x)\n",
    "    y = y.item()\n",
    "    predictions_test_binary_x.append(y)\n",
    "\n",
    "# for subtask B: all dimensions must be saved to be evaluated using evaluation B - for individual evaluation of dimensions use evaluation A\n",
    "#tr = predictions_test_binary_x\n",
    "#ag = predictions_test_binary_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dtf = dtf_test\n",
    "pred_dtf = pred_dtf.drop(columns=[\"TR\", \"AG\"], axis = 1)\n",
    "\n",
    "pred_dtf[\"TR\"] = tr\n",
    "pred_dtf[\"AG\"] = ag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc.\tP (1)\tP (0)\tR (1)\tR (0)\tF1 (1)\tF1 (0)\tP (avg)\tR (avg)\tF1 (avg)\n",
      "0.555\t0.478\t0.657\t0.649\t0.487\t0.551\t0.560\t0.568\t0.568\t0.555\n"
     ]
    }
   ],
   "source": [
    "#SUBTASK A EVAL\n",
    "\n",
    "acc_hs, p_hs, p_nohs, r_hs, r_nohs, f1_hs, f1_nohs, p_macro, r_macro, f1_macro = evaluate_a(predictions_test_binary_x, dtf_test)\n",
    "\n",
    "print(\"\\t\".join([\"{}\".format(x) for x in [\"acc.\", \"P (1)\", \"P (0)\", \"R (1)\", \"R (0)\", \"F1 (1)\", \"F1 (0)\", \"P (avg)\", \"R (avg)\", \"F1 (avg)\"]]))\n",
    "print(\"\\t\".join([\"{0:.3f}\".format(x) for x in [acc_hs, p_hs, p_nohs, r_hs, r_nohs, f1_hs, f1_nohs, p_macro, r_macro, f1_macro]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUBTASK B EVAL\n",
    "macro_f1, emr, acc_levels, p_levels, r_levels, f1_levels = evaluate_b(pred_dtf, dtf_test)\n",
    "\n",
    "print(\"\\t\".join([\"{}\".format(x) for x in [\"acc_HS\", \"acc_TR\", \"acc_AG\", \"p_HS\", \"p_TR\", \"p_AG\", \"r_HS\", \"r_TR\", \"r_AG\", \"f1_HS\", \"f1_TR\", \"f1_AG\", \"emr\", \"macro_f1\"]]))\n",
    "print(\"\\t\".join([\"{0:.3f}\".format(x) for x in [acc_levels[\"HS\"], acc_levels[\"TR\"], acc_levels[\"AG\"], p_levels[\"HS\"], p_levels[\"TR\"], p_levels[\"AG\"], r_levels[\"HS\"], r_levels[\"TR\"], r_levels[\"AG\"], f1_levels[\"HS\"], f1_levels[\"TR\"], f1_levels[\"AG\"], emr, macro_f1]]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c26229294e443791050877fb3bfd11164eaab6d3a6ae053bb24c3c7ccd76f672"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('dlnlp': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
